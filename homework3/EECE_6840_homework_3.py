# EECE 6840 Homework 3
# Created on 9/16/2025
# Created by Mina Gaffney


# Shallow vs. Deep Comparison:
# Train a shallow and deep network on Fashion-MNIST. Experiment with different depths and widths.
# Plot loss, accuracy, and gradient norms across layers.
# Comment on evidence of vanishing/exploding gradients.


# Activation Function Experiments:
# Repeat training with ReLU, LeakyReLU, ELU, and GELU.
# Plot training/validation curves.
# Discuss which activation gave the most stable training and why.


# Batch Normalization and Residuals:
# Add BatchNorm to your deep network and repeat training.
# Add simple residual skip connections.
# Compare convergence speed and final accuracy with/without these modifications.


# Initialization and Optimizers:
# Experiment with Xavier vs. He initialization.
# Train with SGD, Adam, and AdamW.
# Report quantitative differences in convergence and test accuracy.

